# Dialyzer, ETC, and Gradualizer 2

```elixir
Mix.install([
  {:csv, "~> 2.4"},
  {:httpotion, "~> 3.1.0"},
  {:kino_vega_lite, "~> 0.1.1"}
])
```

## Let's get the test data

```elixir
tsv_url =
  "https://gist.githubusercontent.com/erszcz/4d43a77464c87a514e71eecf2811af63/raw/bf8d18cfd236e82bba794c169b57f1f284475ddd/check.2022-07-13_170833.tsv"

response = HTTPotion.get(tsv_url)
```

```elixir
tab = 0x09

{:ok, stream} =
  response.body
  |> StringIO.open()

tsv =
  stream
  |> IO.binstream(:line)
  |> CSV.decode!(separator: tab)
  |> Enum.into([])
  |> Enum.drop(1)
```

```elixir
headers = [
  :type,
  :dialyzer,
  :etc,
  :gradualizer,
  :dialyzer_seconds,
  :etc_seconds,
  :gradualizer_seconds,
  :file
]

# series by header
by_header =
  tsv
  |> Enum.zip()
  |> Enum.map(&Tuple.to_list/1)

by_header = Enum.zip(headers, by_header) |> Enum.into(%{})
```

```elixir
by_test_type = tsv |> Enum.group_by(&List.first/1)

tool_to_index = %{
  dialyzer: 1,
  etc: 2,
  gradualizer: 3
}

extract_tool = fn row, tool ->
  case Enum.at(row, tool_to_index[tool]) do
    "ok" -> [tool |> to_string() |> String.capitalize()]
    _ -> []
  end
end

map_test_type_series_to_tools = fn series ->
  series
  |> Enum.flat_map(fn row ->
    [
      ["All tests"],
      extract_tool.(row, :dialyzer),
      extract_tool.(row, :etc),
      extract_tool.(row, :gradualizer)
    ]
    |> Enum.concat()
  end)
end

by_test_type = %{
  "known_problems_should_fail" =>
    map_test_type_series_to_tools.(by_test_type["known_problems_should_fail"]),
  "known_problems_should_pass" =>
    map_test_type_series_to_tools.(by_test_type["known_problems_should_pass"]),
  "should_fail" => map_test_type_series_to_tools.(by_test_type["should_fail"]),
  "should_pass" => map_test_type_series_to_tools.(by_test_type["should_pass"])
}
```

```elixir
# Sanity check!
[n_should_pass, n_known_problems_should_pass, n_should_fail, n_known_problems_should_fail] = [
  by_test_type["should_pass"] |> Enum.filter(fn e -> e == "All tests" end) |> length,
  by_test_type["known_problems_should_pass"]
  |> Enum.filter(fn e -> e == "All tests" end)
  |> length,
  by_test_type["should_fail"] |> Enum.filter(fn e -> e == "All tests" end) |> length,
  by_test_type["known_problems_should_fail"]
  |> Enum.filter(fn e -> e == "All tests" end)
  |> length
]

[103, 16, 86, 11] = [
  n_should_pass,
  n_known_problems_should_pass,
  n_should_fail,
  n_known_problems_should_fail
]
```

## Analysis and comparison

### Tests that should pass

This chart depicts the number of tests which should pass, i.e. should type check. After all, **we do not want our type checkers to raise warnings about valid code.**

Given we're considering code that's assumed to be valid, none of the type checkers should report warnings here. Cross-checking these tests with different type checkers allows to find bugs in tests or the type checkers themselves.

Dialyzer reports only a few errors. Gradualizer, as expected, reports none. ETC fares quite poorly, passing only 1/3 of the tests, which suggests it might not cover the complete Erlang syntax.

Higher is better.

```elixir
{should_pass_xs, should_pass_ys} =
  by_test_type["should_pass"]
  |> Enum.group_by(fn e -> e end)
  |> Enum.map(fn {type, v} -> {type, length(v)} end)
  |> Enum.unzip()

should_pass = %{
  "Tool" => should_pass_xs,
  "Tests passed" => should_pass_ys
}
```

<!-- livebook:{"attrs":{"chart_title":null,"height":300,"layers":[{"chart_type":"bar","color_field":"Tool","color_field_aggregate":null,"color_field_type":null,"data_variable":"should_pass","x_field":"Tool","x_field_aggregate":null,"x_field_type":null,"y_field":"Tests passed","y_field_aggregate":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":300},"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 300, height: 300)
|> VegaLite.data_from_values(should_pass, only: ["Tool", "Tests passed"])
|> VegaLite.mark(:bar)
|> VegaLite.encode_field(:x, "Tool")
|> VegaLite.encode_field(:y, "Tests passed", type: :quantitative)
|> VegaLite.encode_field(:color, "Tool")
```

### Known problems: tests that should pass, but do not

This chart depicts the number of tests which should pass, i.e. should type check, yet raise Gradualizer errors. In other words, __these are the false positives - invalid or misleading reports about non-issues.__

Dialyzer reports only a single false positive and is a clear winner here! This confirms the slogan that _Dialyzer is never wrong_.
ETC reports some of the errors, but not all of them.
Gradualizer, as expected, reports errors for all of the tests.

Lower is better.

```elixir
{known_problems_should_pass_xs, known_problems_should_pass_ys} =
  by_test_type["known_problems_should_pass"]
  |> Enum.group_by(fn e -> e end)
  |> Enum.map(fn {type, v} -> {type, n_known_problems_should_pass - length(v)} end)
  |> Enum.unzip()

known_problems_should_pass = %{
  "Tool" => known_problems_should_pass_xs ++ ["Gradualizer"],
  "Errors detected" =>
    [n_known_problems_should_pass] ++
      Enum.drop(known_problems_should_pass_ys, 1) ++ [n_known_problems_should_pass]
}
```

<!-- livebook:{"attrs":{"chart_title":null,"height":300,"layers":[{"chart_type":"bar","color_field":"Tool","color_field_aggregate":null,"color_field_type":null,"data_variable":"known_problems_should_pass","x_field":"Tool","x_field_aggregate":null,"x_field_type":null,"y_field":"Errors detected","y_field_aggregate":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":300},"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 300, height: 300)
|> VegaLite.data_from_values(known_problems_should_pass, only: ["Tool", "Errors detected"])
|> VegaLite.mark(:bar)
|> VegaLite.encode_field(:x, "Tool")
|> VegaLite.encode_field(:y, "Errors detected", type: :quantitative)
|> VegaLite.encode_field(:color, "Tool")
```

### Tests which should fail

This chart depicts the number of tests which should fail, i.e. should not type check. **These tests check that the warnings we want to see in our buggy code are actually generated.**

Dialyzer seems to be somewhat permissive. ETC reports the majority of errors. Gradualizer properly reports all the errors.

Higher is better.

```elixir
{should_fail_xs, should_fail_ys} =
  by_test_type["should_fail"]
  |> Enum.group_by(fn e -> e end)
  |> Enum.map(fn {type, v} -> {type, n_should_fail - length(v)} end)
  |> Enum.unzip()

should_fail = %{
  "Tool" => should_fail_xs ++ ["Gradualizer"],
  "Errors detected" => [n_should_fail] ++ Enum.drop(should_fail_ys, 1) ++ [n_should_fail]
}
```

<!-- livebook:{"attrs":{"chart_title":null,"height":300,"layers":[{"chart_type":"bar","color_field":"Tool","color_field_aggregate":null,"color_field_type":null,"data_variable":"should_fail","x_field":"Tool","x_field_aggregate":null,"x_field_type":null,"y_field":"Errors detected","y_field_aggregate":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":300},"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 300, height: 300)
|> VegaLite.data_from_values(should_fail, only: ["Tool", "Errors detected"])
|> VegaLite.mark(:bar)
|> VegaLite.encode_field(:x, "Tool")
|> VegaLite.encode_field(:y, "Errors detected", type: :quantitative)
|> VegaLite.encode_field(:color, "Tool")
```

### Known problems: tests that should fail, but do not

This chart depicts the number of tests which should fail, i.e. should not type check, but are known to type check with Gradualizer. In other words, **these are the errors that considered type checkers cannot find.** We have to rely on tests, code review, or other techniques to find them.

Dialyzer detects some of the errors in these examples, ETC seems to detect even more. Gradualizer doesn't detect any of them, but the examples are crafted against this type checker, so it's expected.

Higher is better.

```elixir
{known_problems_should_fail_xs, known_problems_should_fail_ys} =
  by_test_type["known_problems_should_fail"]
  |> Enum.group_by(fn e -> e end)
  |> Enum.map(fn {type, v} -> {type, n_known_problems_should_fail - length(v)} end)
  |> Enum.unzip()

known_problems_should_fail = %{
  "Tool" => known_problems_should_fail_xs,
  "Errors detected" =>
    [n_known_problems_should_fail] ++ Enum.drop(known_problems_should_fail_ys, 1)
}
```

<!-- livebook:{"attrs":{"chart_title":null,"height":300,"layers":[{"chart_type":"bar","color_field":"Tool","color_field_aggregate":null,"color_field_type":null,"data_variable":"known_problems_should_fail","x_field":"Tool","x_field_aggregate":null,"x_field_type":null,"y_field":"Errors detected","y_field_aggregate":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":300},"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 300, height: 300)
|> VegaLite.data_from_values(known_problems_should_fail, only: ["Tool", "Errors detected"])
|> VegaLite.mark(:bar)
|> VegaLite.encode_field(:x, "Tool")
|> VegaLite.encode_field(:y, "Errors detected", type: :quantitative)
|> VegaLite.encode_field(:color, "Tool")
```

## Some examples

#### should_pass/andalso_any.erl

Described at https://github.com/josefs/Gradualizer/pull/429#discussion_r920465166.
Seems to be a soundness error in Gradualizer!

<!-- livebook:{"break_markdown":true} -->

#### should_pass/binary_exhaustiveness_checking.erl

Dialyzer warns about an unexported function.
Gradualizer type checks functions no matter if they're exported or not.
Dialyzer warning goes away after we export `l/1`.

Fixed for Dialyzer with https://github.com/josefs/Gradualizer/pull/430.

<!-- livebook:{"break_markdown":true} -->

#### should_pass/flow.erl

Dialyzer sees no other local call sites than `bar(apa)`,
therefore infers the argument type of `bar/1` to be just `apa`,
which in turn means that the second clause of `bar/1` is redudant.
This leads to a valid warning.

Adding `-export([bar/1])` at the top makes Dialyzer accept the code.

Gradualizer does not warn either with or without the extra `export` attribute.

Fixed for Dialyzer with https://github.com/josefs/Gradualizer/pull/431.

<!-- livebook:{"break_markdown":true} -->

#### should_pass/iodata.erl

Dialyzer detects an improper list creation, which is not an error for Gradualizer (but maybe it should?) - passing `--infer` to Gradualizer doesn't change the outcome.

<!-- livebook:{"break_markdown":true} -->

#### should_pass/named_fun_infer_pass.erl

Dialyzer infers that 0 in `F(Atoms, 0)` is not a list of integers.
Gradualizer does not.

<!-- livebook:{"break_markdown":true} -->

#### should_pass/named_fun_pass.erl

Dialyzer fails due to arity mismatch.
Gradualizer does so only with `--infer`. Dialyzer is inferring more aggressively.

<!-- livebook:{"break_markdown":true} -->

#### should_pass/negate_none.erl

Dialyzer returns "no local return" from `foo/0`.
Gradualizer is silent.

<!-- livebook:{"break_markdown":true} -->

#### should_pass/records.erl

Dialyzer reported unexported local functions which would never be called.
Gradualizer did not, but the Erlang compiler would, so it's not a big deal.

<!-- livebook:{"break_markdown":true} -->

#### should_pass/return_fun.erl

One of the few cases in `should_pass` tests where Dialyzer actually detects errors,
whereas Gradualizer does not.

```erlang
-spec return_fun_no_spec() -> integer().
return_fun_no_spec() -> fun no_spec/0.

no_spec() -> ok.
```

Dialyzer:

```
return_fun.erl:28:2: Invalid type specification for function return_fun:return_fun_no_spec/0.
The success typing is
          () -> fun(() -> 'ok')
```

ETC:

```
test/should_pass/return_fun.erl: error in parse transform
```

Gradualizer: no error!

Let's remember that by default Gradualizer doesn't infer types of functions with no specs.

Gradualizer with `--infer`:

```
return_fun.erl:29:25: The fun expression is expected to have type integer()
but it has type fun(() -> any())
```

Dialyzer seems to infer more types by default.
However, if we ask Gradualizer to try a bit harder, it's even with Dialyzer on this one.
The file is too complex for ETC to process - it crashes :(

<!-- livebook:{"break_markdown":true} -->

#### should_pass/scope.erl

Another one of the few cases in `should_pass` tests where Dialyzer detects errors,
but Gradualizer does not.

```
f(X) ->
    case g(X) of
        true -> A = 5,
                B = 7;
        false -> B = 6
    end,
    B.

g(_X) ->
    true.
```

Dialyzer:

```
scope.erl:9:9: The pattern 'false' can never match the type 'true'
```

ETC: ok

Gradualizer: ok

Another example showing that Dialyzer type inference is quite aggressive and quite accurate!
It knows that `g/1` can never return anything else than `true`,
so the latter clause of the case expression enclosing the `g(X)` call is redundant.
In this case the entire case expression is redundant and `B` will always be 7.

It's interesting to note that Gradualizer cannot detect the error even if we modify the code as follows:

TODO: create a bug report about it!

```
-spec f(any()) -> atom().
f(X) ->
    case g(X) of
        true -> A = 5,
                B = 7;
        false -> B = 6
    end,
    B.

-spec g(any()) -> true.
g(_X) ->
    true.
```

<!-- livebook:{"break_markdown":true} -->

#### known_problems/should_pass/list_tail.erl

The only false positive raised by Dialyzer, which is known for _never being wrong_. Is it a Dialyzer bug?

```erlang
atom_tail() ->
    [ 1 | list_to_atom("banana") ].
```

Dialyzer:

```
list_tail.erl:7:5: Cons will produce an improper list since its 2nd argument is atom()
```

ETC:

```
unify failed with types [integer] :=: atom
```

Gradualizer:

```
list_tail.erl:7: The expression of type atom() is not a list type
```

Apparently, all the type checkers agree. It definitely is not a Dialyzer bug. Maybe the test should be moved from `known_problems/should_pass` to `should_fail`?

<!-- livebook:{"break_markdown":true} -->

#### known_problems/should_pass/arith_op_arg_types.erl

Gradualizer is wrong about integer type inference in presence of arithmetic operations. Dialyzer is fine with it.

<!-- livebook:{"break_markdown":true} -->

#### known_problems/should_pass/binary_exhaustiveness_checking_should_pass.erl

Gradualizer cannot tell that `<<_:24>>` is a subtype of `<<_:_*8>>`.

<!-- livebook:{"break_markdown":true} -->

#### should_fail/annotated_types_fail.erl

Gradualizer provides type annotation/assertion macros which can be used to provide extra type information by the programmer. Gradualizer correctly finds types discrepancies between specs and annotations.

Dialyzer does not understand the annotation macros and does not have enough info to find type errors. Dialyzer is more permissive.

<!-- livebook:{"break_markdown":true} -->

#### should_fail/branch.erl

Given the code:

```
-spec c(boolean()) -> integer().
c(X) ->
    X.
```

Dialyzer (with no extra flags) does not detect type mismatch. With `-Wspecdiffs` or `-Woverspecs` it reports:

```
branch.erl:5:2: Type specification branch:c
          (boolean()) -> integer() is a subtype of the success typing: branch:c
          (_) -> any()
```

The report is correct. Arguably, though, it's a bit hard to understand.

Gradualizer reports:

```
The variable is expected to have type integer() but it has type false | true
```

<!-- livebook:{"break_markdown":true} -->

#### should_fail/branch2.erl

Given the code:

```
-spec c(boolean()) -> integer().
c(true) ->
    1;
c(false) ->
    apa.
```

Dialyzer (with no options) does not detect type mismatch. With `-Wspecdiffs` it reports:

```
branch2.erl:5:2: The success typing for branch2:c/1 implies that the function might also return
          'apa' but the specification return is
          integer()
```

Which is expected.

Gradualizer by default reports:

```
The atom is expected to have type integer() but it has type apa:

    1;
c(false) ->
    apa.
    ^^^
```

<!-- livebook:{"break_markdown":true} -->

#### should_fail/case_pattern.erl

Given the code:

```
-spec f(integer(), atom()) -> ok.
f(X, Y) ->
    case Y of
        X -> ok
    end.
```

Dialyzer (with no options) cannot figure out that the pattern (matching integers) will never match an atom. With `-Wspecdiffs` a misleading warning is printed:

```
case_pattern.erl:4:2: Type specification case_pattern:f
          (integer(), atom()) -> 'ok' is a subtype of the success typing: case_pattern:f
          (_, _) -> 'ok'
```

The warning suggests that our spec is too specific, whereas in practice its explicitness enables the type checker to figure out that the case expression will never match.

Gradualizer reports:

```
The variable is expected to have type atom() but it has type integer():

f(X, Y) ->
    case Y of
        X -> ok
        ^
```

At runtime, the code fails with:

```
2> case_pattern:f(3, a).
** exception error: no case clause matching a
     in function  case_pattern:f/2 (test/should_fail/case_pattern.erl, line 6)
```

So Gradualizer detects an actual bug that Dialyzer does not.

<!-- livebook:{"break_markdown":true} -->

#### should_fail/case_pattern2.erl

Given the code:

```
-spec f(integer(), atom()) -> ok.
f(X, Y) ->
    case {X, Y} of
        {Z, Z} -> ok
    end.
```

Dialyzer (with no options) cannot figure out that the pattern will never match the expression as X and Y are of different types. With `-Wspecdiffs` it suggests the spec is too strict:

```
case_pattern2.erl:4:2: Type specification case_pattern2:f
          (integer(), atom()) -> 'ok' is a subtype of the success typing: case_pattern2:f
          (_, _) -> 'ok'
```

Gradualizer reports that:

```
The variable is expected to have type atom() but it has type integer():

f(X, Y) ->
    case {X, Y} of
        {Z, Z} -> ok
            ^
```

So Gradualizer prevents the runtime error:

```
2> case_pattern2:f(3, a).
** exception error: no case clause matching {3,a}
     in function  case_pattern2:f/2 (test/should_fail/case_pattern2.erl, line 6)
```

<!-- livebook:{"break_markdown":true} -->

#### should_fail/covariant_map_keys_fail.erl

Given the code:

```
-spec good(#{ good := A }) -> A.
good(#{ good := X }) -> X.

-spec not_good(#{good | bad := A}) -> A.
not_good(M) -> good(M). %% This call should fail

-spec kaboom() -> integer().
kaboom() -> not_good(#{ bad => 0 }).
```

Dialyzer (with no options) does not detect any errors. With `-Wspecdiffs` it reports:

```
covariant_map_keys_fail.erl:6:2: Type specification covariant_map_keys_fail:good
          (#{'good' := A}) -> A is a subtype of the success typing: covariant_map_keys_fail:good
          (#{'good' := _, _ => _}) -> any()
covariant_map_keys_fail.erl:9:2: Type specification covariant_map_keys_fail:not_good
          (#{'good' | 'bad' := A}) -> A is a supertype of the success typing: covariant_map_keys_fail:not_good
          (#{'good' := _}) -> any()
covariant_map_keys_fail.erl:12:2: The specification for covariant_map_keys_fail:kaboom/0 states that the function might also return
          integer() but the inferred return is
          none()
```

Dialyzer suggests the information we pass in `good/1` spec is redundant. It hints at an error in `not_good/1`, but is not explicit about it. It correctly detects an error in `kaboom/0`.

Gradualizer reports:

```
The variable is expected to have type #{good := A} but it has type #{bad | good := A}

-spec not_good(#{good | bad := A}) -> A.
not_good(M) -> good(M).
                    ^
```

Gradualizer does not detect the error in `kaboom/0`, but it does detect the more local error in `not_good/1`. If we fix the error in `not_good/1`, the error in `kaboom/0` will automatically be fixed, too.

However, Gradualizer, even with the `--infer` flag, does not infer the type of `#{ bad => 0 }` expression passed into `not_good/1` as an argument. This is now tracked as https://github.com/josefs/Gradualizer/issues/432.

Interestingly, if we modify the code to:

```
-spec kaboom() -> integer().
kaboom() ->
    M = #{ bad => 0 },
    not_good(M).
```

Gradualizer is able to report also this error:

```
The variable is expected to have type #{good | bad := A} but it has type #{bad => 0}

kaboom() ->
    M = #{ bad => 0 },
    not_good(M).
             ^
```

The runtime error Gradualizer protects from is:

```
2> covariant_map_keys_fail:not_good(#{bad => 0}).
** exception error: no function clause matching covariant_map_keys_fail:good(#{bad => 0}) (test/should_fail/covariant_map_keys_fail.erl, line 7)
```

<!-- livebook:{"break_markdown":true} -->

#### should_fail/cyclic_type_vars.erl

Given the code:

```
-spec foo(A) -> B when
    A :: true | B,
    B :: [A].
```

Gradualizer reports a cyclic dependency between type variables A and B. Dialyzer does not.

<!-- livebook:{"break_markdown":true} -->

#### should_fail/exhaustive.erl

Dialyzer (with no options) does not fail type checking the file. With `-Wspecdiffs` it hints at spec and implementation mismatches, but with not clear indications which are wrong.

Gradualizer assumes the specs to be right and therefore returns explicit exhaustiveness failures:

```
/Users/erszcz/work/erszcz/gradualizer/test/should_fail/exhaustive.erl:48:1: Nonexhaustive patterns: 0
```

It becomes even more apparent when we add an example of a nonexhaustive `case` expression inside a function body:

```
-spec integer_2(integer()) -> {}.
integer_2(I) ->
    case I of
        0 -> {}
    end.
```

Gradualizer reports the exact line:

```
should_fail/exhaustive.erl:53:9: Nonexhaustive patterns: -1
```

Whereas Dialyzer (with `-Wspecdiffs`) only reports a hint that our passed in type is wider than the implementation expects:

```
exhaustive.erl:50:2: Type specification exhaustive:integer_2
          (integer()) -> {} is a supertype of the success typing: exhaustive:integer_2
          (0) -> {}
```
